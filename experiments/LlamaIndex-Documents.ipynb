{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "import openai\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "#from llama_index.vector_stores import ElasticsearchStore\n",
    "#from llama_index.vector_stores.elasticsearch.base import ElasticsearchStore\n",
    "from llama_index.vector_stores.elasticsearch import ElasticsearchStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"pdfs\").load_data()\n",
    "print(f\"Loaded {len(documents)} document(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we're setting up the OpenAI API key and initializing a `SimpleNodeParser`. This parser processes our list of `Document` objects into 'nodes', which are the basic units that `llama_index` uses for indexing and querying. The first node is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SentenceSplitter(chunk_size=1024) #  SentenceSplitter\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connecting to vector db of elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import GPTVectorStoreIndex, StorageContext, ServiceContext\n",
    "\n",
    "elasticsearch_endpoint_url = \"http://localhost:9201\"  \n",
    "index_name = \"semantic-chunk\"  \n",
    "\n",
    "# Setup for the Elasticsearch vector store\n",
    "vector_store = ElasticsearchStore(\n",
    "    index_name=index_name, \n",
    "    es_url=elasticsearch_endpoint_url,\n",
    ")\n",
    "\n",
    "# Setup the storage context with the vector store\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# setup the index/query process, ie the embedding model\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model='text-embedding-ada-002', \n",
    "    embed_batch_size=100) # That means I send less requests to the API. So it's faster and cheaper. Netrowk latency is the bottleneck here.\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "\n",
    "# now the below is out indexing pipeline\n",
    "index = GPTVectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context,\n",
    "    service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
    "# Create the index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    similarity_top_k=3)\n",
    "index.storage_context.persist(persist_dir=\"<persist_dir>\")\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.70)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "response = query_engine.query(\"What are conditions for Paternity Leave?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "print(response.source_nodes[idx])\n",
    "print(response.source_nodes[idx].node.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response_and_sources(response):\n",
    "    # First, print the response object directly\n",
    "    print(\"Response:\")\n",
    "    print(response)\n",
    "    print(\"\\nSources:\")\n",
    "\n",
    "    # Initialize a counter for source numbering\n",
    "    source_counter = 1\n",
    "\n",
    "    # Check if the response object has the attribute 'source_nodes' to avoid errors\n",
    "    if hasattr(response, 'source_nodes'):\n",
    "        for node in response.source_nodes:\n",
    "            # Access the metadata for the current node\n",
    "            metadata = node.node.metadata\n",
    "            file_name = metadata.get('file_name', 'N/A')  # Default to 'N/A' if not found\n",
    "            page_label = metadata.get('page_label', 'N/A')  # Default to 'N/A' if not found\n",
    "            \n",
    "            # Replace single newlines with spaces, and double newlines with a single newline\n",
    "            formatted_text = node.node.text.replace('\\n\\n', '\\u2028').replace('\\n', ' ').replace('\\u2028', '\\n\\n')\n",
    "            print(f\"Source{source_counter} (File: {file_name}, Page: {page_label}):\")\n",
    "            print(formatted_text)\n",
    "            print(\"\\n\")\n",
    "            source_counter += 1\n",
    "    else:\n",
    "        print(\"No source nodes found in response.\")\n",
    "\n",
    "# Assuming `response` is your object, you would call the function like this:\n",
    "print_response_and_sources(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (RelevancyEvaluator) --RelevancyEvaluator to measure if the response + source nodes match the query. This is useful for measuring if the query was actually answered by the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator, ResponseEvaluator, QueryResponseEvaluator, RelevancyEvaluator\n",
    "from llama_index.core import (SimpleDirectoryReader, \n",
    "                         ServiceContext,  \n",
    "                         GPTVectorStoreIndex, \n",
    "                         load_index_from_storage, \n",
    "                         StorageContext,\n",
    "                         Response)\n",
    "#from llama_index.core import LLMPredictor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.evaluation import EvaluationResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question generation\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"pdfs\").load_data()\n",
    "\n",
    "#set up llm model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "#generate questions\n",
    "data_generator = DatasetGenerator.from_documents(documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\") # also load_index_from_storage might be used\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "#now query the index\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.70)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(eval_questions[0])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "evaluator = RelevancyEvaluator(llm=gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_engine = index.as_query_engine()\n",
    "response_vector = query_engine.query(eval_questions[0])\n",
    "eval_result = evaluator.evaluate_response(\n",
    "    query=eval_questions[0], response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(\n",
    "    query: str, response: Response, eval_result: EvaluationResult\n",
    ") -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Query\": query,\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
    "            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "            \"Reasoning\": eval_result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)\n",
    "\n",
    "# display evaluation result\n",
    "display_eval_df(eval_questions[0], response_vector, eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eval_data = []\n",
    "# Loop through all questions in eval_questions\n",
    "for question in eval_questions:\n",
    "    response_vector = query_engine.query(question)\n",
    "    # Evaluate the response\n",
    "    eval_result = evaluator.evaluate_response(\n",
    "        query=question, response=response_vector\n",
    "    )\n",
    "    current_eval_data = {\n",
    "        \"Query\": question,\n",
    "        \"Response\": str(response_vector),\n",
    "        \"Source\": response_vector.source_nodes[0].node.text[:1000] + \"...\",\n",
    "        \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "        \"Reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    eval_data.append(current_eval_data)\n",
    "    current_eval_df = pd.DataFrame([current_eval_data])\n",
    "    current_eval_df_styled = current_eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    \n",
    "    display(current_eval_df_styled)\n",
    "final_eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = \"evaluation_results.csv\"\n",
    "final_eval_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Saved evaluation results to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval_df[\"Evaluation Result\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (RetrieverEvaluator)-- Failed For Now because of how the retriever is set up in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "#retriever = index.as_retriever(similarity_top_k=3)\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(\"how many weeks allowed for Paternity Leave?\")\n",
    "\n",
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes,\n",
    "    llm=gpt3,\n",
    "    num_questions_per_chunk=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = qa_dataset.queries.values()\n",
    "print(list(queries)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mrr\", \"hit_rate\"]\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out on a sample query\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[5]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
