{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "import openai\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.vector_stores.elasticsearch import ElasticsearchStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 document(s).\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"/Users/halilergul/Desktop/simpplr-v3/pdfs\").load_data()\n",
    "print(f\"Loaded {len(documents)} document(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we're setting up the OpenAI API key and initializing a `SimpleNodeParser`. This parser processes our list of `Document` objects into 'nodes', which are the basic units that `llama_index` uses for indexing and querying. The first node is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\",  embed_batch_size=100) # \n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.node_parser.text.semantic_splitter.SemanticSplitterNodeParser"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connecting to vector db of elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import GPTVectorStoreIndex, StorageContext, ServiceContext\n",
    "from llama_index.core import Settings\n",
    "\n",
    "elasticsearch_endpoint_url = \"http://localhost:9201\"  # \n",
    "index_name = \"simpplr-policy-dlkfjsçdödçksljeroıruehfoıeho\"  # \n",
    "\n",
    "# Setup for the Elasticsearch vector store. This is where the vectors will be stored.\n",
    "vector_store = ElasticsearchStore(\n",
    "    index_name=index_name, \n",
    "    es_url=elasticsearch_endpoint_url,\n",
    ")\n",
    "\n",
    "# Setup the storage context with the vector store\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# setup the index/query process, ie the embedding model (and completion if used)\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model='text-embedding-3-small', \n",
    "    embed_batch_size=100) # That means I send less requests to the API. So it's faster and cheaper. Netrowk latency is the bottleneck here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
    "# Create the index\n",
    "# noods usage: https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/\n",
    "#service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    "    #service_context=service_context,\n",
    ")\n",
    "#index.storage_context.persist(persist_dir=\"<persist_dir>\")\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.70)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fathers and non-birthing parents are eligible for up to 4 weeks of paid paternity leave.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are conditions for Paternity Leave and how many weeks I have?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 7e779480-0fc4-4d41-80d2-517ffbc29fba\n",
      "Text: Intend to return to work after the leave period is over. 3.\n",
      "Provide at least four weeks' notice, except in cases of emergencies or\n",
      "unforeseen circumstances. Types of Parental Leave: 1. Maternity Leave:\n",
      "Birthing mothers are entitled to take up to 12 weeks of paid maternity\n",
      "leave. 2. Paternity Leave: Fathers and non-birthing parents are\n",
      "eligible f...\n",
      "Score:  1.000\n",
      "\n",
      "{'page_label': '1', 'file_name': 'GPT-parental leave policy.pdf', 'file_path': '/Users/halilergul/Desktop/simpplr-v3/pdfs/GPT-parental leave policy.pdf', 'file_type': 'application/pdf', 'file_size': 36062, 'creation_date': '2024-04-11', 'last_modified_date': '2024-03-18'}\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(response.source_nodes[idx])\n",
    "print(response.source_nodes[idx].node.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Fathers and non-birthing parents are eligible for up to 4 weeks of paid paternity leave.\n",
      "\n",
      "Sources:\n",
      "Source1 (File: GPT-parental leave policy.pdf, Page: 1):\n",
      "Intend to return to work after the leave period is over. 3. Provide at least four weeks' notice, except in cases of emergencies or unforeseen circumstances. Types of Parental Leave: 1. Maternity Leave: Birthing mothers are entitled to take up to 12 weeks of paid maternity leave. 2. Paternity Leave: Fathers and non-birthing parents are eligible for up to 4 weeks of paid leave. 3. Adoption/Foster Care Leave: Employees who adopt or foster a child are entitled to up to 4 weeks of paid leave. Parental Leave Benefits: 1. Paid Leave Benefits: a. Maternity Leave: During the 12 weeks of paid maternity leave, employees will receive their normal base salary. b. Paternity Leave: During the 4 weeks of paid paternity leave, employees will receive their normal base salary. c. Adoption/Foster Care Leave: During the 4 weeks of paid adoption/foster care leave, employees will receive their normal base salary. 2. Unpaid Leave: a. In addition to the paid parental leave, employees may request an extended period of unpaid leave, up to a maximum of 12 additional weeks, beyond the paid leave entitlement. b. During unpaid leave, employees will maintain their eligibility for approved benefits, such as health insurance, subject to any required employee contributions.\n",
      "\n",
      "\n",
      "Source2 (File: GPT-parental leave policy.pdf, Page: 2):\n",
      "Medical Certification: a. Maternity leave requests should be accompanied by medical certification confirming the due date as well as the length of time taken for recovery. b. All adoption and foster care leave requests should be supported by documentation outlining the process, anticipated placement date, and leave start date. 3. Return to Work: a. Employees on parental leave must provide a written notice of intent to return to work at least two weeks before the scheduled end date of the leave. b. Simpplr will make reasonable efforts to reinstate employees in their previous position or an equivalent position, subject to operational requirements. Conclusion: Simpplr's Parental Leave Policy seeks to support employees in balancing their work and family responsibilities during this important time in their lives. By offering paid leave and the flexibility to take unpaid leave, we aim to create an inclusive and supportive workplace that values and respects the needs of growing families. This policy reflects our commitment to the well-being of our employees and their families while promoting a positive work-life balance.\n",
      "\n",
      "\n",
      "Source3 (File: GPT-parental leave policy.pdf, Page: 2):\n",
      "Leave Process and Documentation: 1. Notice of Leave: a. Employees seeking parental leave must notify their immediate supervisor and Human Resources at least four weeks in advance, or as soon as reasonably possible. b. Employees should submit a written request stating the anticipated start and end dates of the leave, as well as any additional information required by Simpplr. 2. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_response_and_sources(response):\n",
    "    # First, print the response object directly\n",
    "    print(\"Response:\")\n",
    "    print(response)\n",
    "    print(\"\\nSources:\")\n",
    "\n",
    "    # Initialize a counter for source numbering\n",
    "    source_counter = 1\n",
    "    if hasattr(response, 'source_nodes'):\n",
    "        for node in response.source_nodes:\n",
    "            metadata = node.node.metadata\n",
    "            file_name = metadata.get('file_name', 'N/A')  # Default to 'N/A' if not found\n",
    "            page_label = metadata.get('page_label', 'N/A')  # Default to 'N/A' if not found\n",
    "            formatted_text = node.node.text.replace('\\n\\n', '\\u2028').replace('\\n', ' ').replace('\\u2028', '\\n\\n')\n",
    "            print(f\"Source{source_counter} (File: {file_name}, Page: {page_label}):\")\n",
    "            print(formatted_text)\n",
    "            print(\"\\n\") \n",
    "            source_counter += 1\n",
    "    else:\n",
    "        print(\"No source nodes found in response.\")\n",
    "print_response_and_sources(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The referral bonus is paid out in two parts.\n"
     ]
    }
   ],
   "source": [
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context#, service_context=service_context\n",
    ")\n",
    "\n",
    "# create a query engine\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"How much parts is the referral bonus paid out?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  if you are using a vector index, you can get the similarity scores for each node used to create the response. In my experience, anything over 0.77 is usually a good sign \n",
    "\n",
    "response.source_nodes[0].score will get the score of the first source node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (RelevancyEvaluator) --RelevancyEvaluator to measure if the response + source nodes match the query. This is useful for measuring if the query was actually answered by the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator, ResponseEvaluator, QueryResponseEvaluator, RelevancyEvaluator\n",
    "from llama_index.core import (SimpleDirectoryReader, \n",
    "                         ServiceContext,  \n",
    "                         GPTVectorStoreIndex, \n",
    "                         load_index_from_storage, \n",
    "                         StorageContext,\n",
    "                         Response)\n",
    "#from llama_index.core import LLMPredictor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.evaluation import EvaluationResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question generation\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"pdfs\").load_data()\n",
    "\n",
    "#set up llm model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.node_parser = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    ")\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "#generate questions\n",
    "data_generator = DatasetGenerator.from_documents(documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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' \n",
    "gpt3 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# reload the index from the vector store\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\") # also load_index_from_storage might be used\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "#now query the index\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.70)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "evaluator = RelevancyEvaluator(llm=gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are conditions for Paternity Leave?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    elasticsearch_endpoint_url)\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_engine = index.as_query_engine()\n",
    "response_vector = query_engine.query(eval_questions[0])\n",
    "eval_result = evaluator.evaluate_response(\n",
    "    query=eval_questions[0], response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(\n",
    "    query: str, response: Response, eval_result: EvaluationResult\n",
    ") -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Query\": query,\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
    "            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "            \"Reasoning\": eval_result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)\n",
    "\n",
    "# display evaluation result\n",
    "display_eval_df(eval_questions[0], response_vector, eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_data = []\n",
    "for question in eval_questions:\n",
    "    response_vector = query_engine.query(question)\n",
    "    # Evaluate the response\n",
    "    eval_result = evaluator.evaluate_response(\n",
    "        query=question, response=response_vector\n",
    "    )\n",
    "    \n",
    "    # Append the relevant data to the eval_data list\n",
    "    current_eval_data = {\n",
    "        \"Query\": question,\n",
    "        \"Response\": str(response_vector),\n",
    "        \"Source\": response_vector.source_nodes[0].node.text[:1000] + \"...\",\n",
    "        \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "        \"Reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    eval_data.append(current_eval_data)\n",
    "    current_eval_df = pd.DataFrame([current_eval_data])\n",
    "    current_eval_df_styled = current_eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(current_eval_df_styled)\n",
    "final_eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = \"evaluation_results.csv\"\n",
    "final_eval_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Saved evaluation results to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval_df[\"Evaluation Result\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
