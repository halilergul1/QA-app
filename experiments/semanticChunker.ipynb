{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import getpass\n",
    "import openai\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.vector_stores.elasticsearch import ElasticsearchStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"pdfs\").load_data()\n",
    "print(f\"Loaded {len(documents)} document(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we're setting up the OpenAI API key and initializing a `SimpleNodeParser`. This parser processes our list of `Document` objects into 'nodes', which are the basic units that `llama_index` uses for indexing and querying. The first node is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\",  embed_batch_size=100) # \n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connecting to vector db of elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import GPTVectorStoreIndex, StorageContext, ServiceContext\n",
    "from llama_index.core import Settings\n",
    "\n",
    "elasticsearch_endpoint_url = \"http://localhost:9201\"  # Or your actual Elasticsearch endpoint URL\n",
    "index_name = \"simpplr-policy-qa\"  # The name of the Elasticsearch index you want to use or create\n",
    "\n",
    "# Setup for the Elasticsearch vector store. This is where the vectors will be stored.\n",
    "vector_store = ElasticsearchStore(\n",
    "    index_name=index_name, \n",
    "    es_url=elasticsearch_endpoint_url,\n",
    ")\n",
    "\n",
    "# Setup the storage context with the vector store\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# setup the index/query process, ie the embedding model (and completion if used)\n",
    "embed_model = OpenAIEmbedding(\n",
    "    model='text-embedding-3-small', \n",
    "    embed_batch_size=100) # That means you send less requests to the API. So it's faster and cheaper. Netrowk latency is the bottleneck here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.llamaindex.ai/en/stable/understanding/querying/querying/\n",
    "# Create the index\n",
    "# noods usage: https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context\n",
    ")\n",
    "#index.storage_context.persist(persist_dir=\"<persist_dir>\")\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.70)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are conditions for Paternity Leave?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(response.source_nodes[idx])\n",
    "print(response.source_nodes[idx].node.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response_and_sources(response):\n",
    "    # First, print the response object directly\n",
    "    print(\"Response:\")\n",
    "    print(response)\n",
    "    print(\"\\nSources:\")\n",
    "\n",
    "    # Initialize a counter for source numbering\n",
    "    source_counter = 1\n",
    "\n",
    "    # Check if the response object has the attribute 'source_nodes' to avoid errors\n",
    "    if hasattr(response, 'source_nodes'):\n",
    "        for node in response.source_nodes:\n",
    "            # Access the metadata for the current node\n",
    "            metadata = node.node.metadata\n",
    "            file_name = metadata.get('file_name', 'N/A')  # Default to 'N/A' if not found\n",
    "            page_label = metadata.get('page_label', 'N/A')  # Default to 'N/A' if not found\n",
    "            \n",
    "            # Replace single newlines with spaces, and double newlines with a single newline\n",
    "            # This attempts to maintain paragraph breaks while removing unnecessary line breaks\n",
    "            formatted_text = node.node.text.replace('\\n\\n', '\\u2028').replace('\\n', ' ').replace('\\u2028', '\\n\\n')\n",
    "            \n",
    "            # Print each source with its number and metadata in parentheses\n",
    "            print(f\"Source{source_counter} (File: {file_name}, Page: {page_label}):\")\n",
    "            print(formatted_text)\n",
    "            print(\"\\n\")  # Add extra newline for better separation between sources\n",
    "            \n",
    "            # Increment the source counter for the next source\n",
    "            source_counter += 1\n",
    "    else:\n",
    "        print(\"No source nodes found in response.\")\n",
    "\n",
    "# Assuming `response` is your object, you would call the function like this:\n",
    "print_response_and_sources(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# create a query engine\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What are conditions for remote work?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  if you are using a vector index, you can get the similarity scores for each node used to create the response. In my experience, anything over 0.77 is usually a good sign \n",
    "\n",
    "response.source_nodes[0].score will get the score of the first source node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (RelevancyEvaluator) --RelevancyEvaluator to measure if the response + source nodes match the query. This is useful for measuring if the query was actually answered by the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator, ResponseEvaluator, QueryResponseEvaluator, RelevancyEvaluator\n",
    "from llama_index.core import (SimpleDirectoryReader, \n",
    "                         ServiceContext,  \n",
    "                         GPTVectorStoreIndex, \n",
    "                         load_index_from_storage, \n",
    "                         StorageContext,\n",
    "                         Response)\n",
    "#from llama_index.core import LLMPredictor\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.evaluation import EvaluationResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question generation\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"pdfs\").load_data()\n",
    "\n",
    "#set up llm model\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.node_parser = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    ")\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "#generate questions\n",
    "data_generator = DatasetGenerator.from_documents(documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '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' \n",
    "gpt3 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# reload the index from the vector store\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\") # also load_index_from_storage might be used\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "#now query the index\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.70)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluator\n",
    "evaluator = RelevancyEvaluator(llm=gpt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are conditions for Paternity Leave?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    elasticsearch_endpoint_url)\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_engine = index.as_query_engine()\n",
    "response_vector = query_engine.query(eval_questions[0])\n",
    "eval_result = evaluator.evaluate_response(\n",
    "    query=eval_questions[0], response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(\n",
    "    query: str, response: Response, eval_result: EvaluationResult\n",
    ") -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Query\": query,\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
    "            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "            \"Reasoning\": eval_result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)\n",
    "\n",
    "# display evaluation result\n",
    "display_eval_df(eval_questions[0], response_vector, eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to hold all the evaluation data\n",
    "eval_data = []\n",
    "\n",
    "# Loop through all questions in eval_questions\n",
    "for question in eval_questions:\n",
    "    # Query the index\n",
    "    response_vector = query_engine.query(question)\n",
    "    \n",
    "    # Evaluate the response\n",
    "    eval_result = evaluator.evaluate_response(\n",
    "        query=question, response=response_vector\n",
    "    )\n",
    "    \n",
    "    # Append the relevant data to the eval_data list\n",
    "    current_eval_data = {\n",
    "        \"Query\": question,\n",
    "        \"Response\": str(response_vector),\n",
    "        \"Source\": response_vector.source_nodes[0].node.text[:1000] + \"...\",\n",
    "        \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "        \"Reasoning\": eval_result.feedback,\n",
    "    }\n",
    "    eval_data.append(current_eval_data)\n",
    "\n",
    "    # Create a temporary DataFrame for the current evaluation data\n",
    "    current_eval_df = pd.DataFrame([current_eval_data])\n",
    "    \n",
    "    # Optionally adjust the display properties for better readability\n",
    "    current_eval_df_styled = current_eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    \n",
    "    # Display the current evaluation result\n",
    "    display(current_eval_df_styled)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "final_eval_df = pd.DataFrame(eval_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_filename = \"evaluation_results.csv\"\n",
    "final_eval_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Saved evaluation results to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_eval_df[\"Evaluation Result\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
